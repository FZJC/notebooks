{
  "metadata" : {
    "id" : "81c89972-6415-4fac-ad50-72dcfc5f7587",
    "name" : "enriching-streaming-data",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "40AAFF23B9654D2A9B2E0C59515BAF8A"
    },
    "cell_type" : "markdown",
    "source" : "# Enriching Streaming Data"
  }, {
    "metadata" : {
      "id" : "D5BA242BD89A48F3816A0D7F53206849"
    },
    "cell_type" : "markdown",
    "source" : "In this notebook we start where our previous example left us:\n\nWe have a process that is able to generate random data and we created a streaming job that is able to consume, \nparse and save this data to a parquet file.\n\nWe are now interested in loading a parquet file containig the configuration for each sensor in our system and use that data to enrich the incoming sensor data."
  }, {
    "metadata" : {
      "id" : "79CD566650664F04A8515F55A809A44D"
    },
    "cell_type" : "markdown",
    "source" : "## Our Streaming dataset will consist of sensor information, containing the sensorId, a timestamp, and a value.\nFor the sake of simplicity in this self-contained example, we are going to generate a randomized dataset, using an scenario that simulates a real IoT use case.\nThe timestamp will be the time of execution and each record will be formatted as a string coming from \"the field\" of comma separated values.\n\nWe also add a bit of real-world chaos to the data: Due to weather conditions, some sensors publish corrupt data. "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "1BA7507DA513491F8D3A1AED38F087CB"
    },
    "cell_type" : "code",
    "source" : [ "val sensorCount = 100000\n", "val workDir = \"/tmp/learningsparkstreaming/\"\n", "val referenceFile = \"sensor-records.parquet\"\n", "val targetFile = \"enrichedIoTStream.parquet\"" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9CA4FD2CB34F47AF8097D5E2A2BF97DA"
    },
    "cell_type" : "code",
    "source" : [ "import scala.util.Random\n", "val sensorId: () => Int = () =>  Random.nextInt(sensorCount)\n", "val data: () => Double = () => Random.nextDouble\n", "val timestamp: () => Long = () => System.currentTimeMillis\n", "val recordFunction: () => String = { () => \n", "                                      if (Random.nextDouble < 0.9) {\n", "                                        Seq(sensorId().toString, timestamp(), data()).mkString(\",\")\n", "                                      } else {\n", "                                        \"!!~corrupt~^&##$\"                                   \n", "                                      }\n", "                                   }" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "5806B402E3314F15A0B21F46BC97FF4A"
    },
    "cell_type" : "markdown",
    "source" : "### We use a particular trick that requires a moment of attention\nInstead of creating an RDD of text records, we create an RDD of record-generating functions. \nThen, each time the RDD is evaluated, the record function will generate a new random record. \nThis way we can simulate a realistic load of data that delivers a different set on each batch."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A9AADE76D9DA471B9708B80B3E765890"
    },
    "cell_type" : "code",
    "source" : [ "val sensorDataGenerator = sparkContext.parallelize(1 to 100).map(_ => recordFunction)\n", "val sensorData = sensorDataGenerator.map(recordFun => recordFun())" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "62A5D12E89B24EFB8C411495942032B1"
    },
    "cell_type" : "markdown",
    "source" : "# Load the reference data from a parquet file\nWe also cache the data to keep it in memory and improve the performance of our steaming application"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "78E6ABA2D88B47F38905441B7D79E65D"
    },
    "cell_type" : "code",
    "source" : [ "val sensorRef = sparkSession.read.parquet(s\"$workDir/$referenceFile\")\n", "sensorRef.cache()" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "9CBAE4C0C1EB48DBB802A0CC2960B2DB"
    },
    "cell_type" : "markdown",
    "source" : "(Parquet files preserve the schema information, which we can retrieve from the DataFrame)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab484749271-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "774E09B823774494979FC8FE6AE3078F"
    },
    "cell_type" : "code",
    "source" : [ "sensorRef.schema" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "BFF9E80BAB5C4D468154BCD6FB5C6501"
    },
    "cell_type" : "markdown",
    "source" : "## We create our Streaming Context"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "83B2B58C552240BBA6FF518B1AD274EB"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.streaming.StreamingContext\n", "import org.apache.spark.streaming.Seconds\n", "\n", "@transient val streamingContext = new StreamingContext(sparkContext, Seconds(2))" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "4E6DBF82BB6F4B6584233CD460F67263"
    },
    "cell_type" : "markdown",
    "source" : "## Our stream source will be a ConstantInputDStream fed by the record-generating RDD.\nBy combining a constant input DStream with the record generating RDD, we create a changing stream of data to process in our example.\n(This method makes the example self-contained. It removes the need of an external stream generating process)\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DF03F66BDDE0447B8202D39F2C0202E2"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark.streaming.dstream.ConstantInputDStream\n", "val rawDStream  = new ConstantInputDStream(streamingContext, sensorData)\n" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "CCCB597031E7451FB59D18BA85C0E4A4"
    },
    "cell_type" : "markdown",
    "source" : "# Providing Schema information for our streaming data\nNow that we have a DStream of fresh data processed in a 2-second interval, we can start focusing on the gist of this example.\nFirst, we want to define and apply a schema to the data we are receiving.\nIn Scala, we can define a schema with a `case class`"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E7A917C393654969812E6E38223BBA52"
    },
    "cell_type" : "code",
    "source" : [ "case class SensorData(sensorId: Int, timestamp: Long, value: Double)" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "9AD1ACAD450E44DA8C046EB48CD4EE5A"
    },
    "cell_type" : "markdown",
    "source" : "Now we apply that schema to the dstream, using the `flatMap` function.\n\nWe use `flatMap` instead of a `map` because there might be cases when the incoming data is incomplete or corrupted.\nIf we would use `map`, we would have to provide a resulting value for each transformed record. \nThat is something we cannot do for invalid records.\nWith `flatMap` in combination with `Option`, we can represent valid records as `Some(recordValue)` and invalid records as `None`.\nBy the virtue of `flatMap` the internal `Option` container gets flattend and our resulting stream will only contain valid `recordValue`s.\n\nDuring the parsing of the comma separated records, we not only protect ourselves against missing fields, but also parse the numeric values to their expected types. The surrounding `Try` captures any `NumberFormatException` that might arise from invalid records."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "5285C2BBC1854F059AB8E1D0244AE1C7"
    },
    "cell_type" : "code",
    "source" : [ "import scala.util.Try\n", "val schemaStream = rawDStream.flatMap{record => \n", "                                  val fields = record.split(\",\")\n", "                                  if (fields.size == 3) {\n", "                                    Try (SensorData(fields(0).toInt, fields(1).toLong, fields(2).toDouble)).toOption\n", "                                  } else { None }\n", "                                 }" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "7A7C144384904E96BE66A649BD193C15"
    },
    "cell_type" : "markdown",
    "source" : "# Enrich the streaming data\nWith the schema stream in place, we can proceed to transform the underlying RDDs in DataFrames.\nThis time, we are going to use the reference data to add the specific sensor information. \n\nWe are also going to de-normalize the recorded value according to the sensor range so that we don't have to repeat that data in the resulting dataset.\n\nAs before, we do this in the context of the general-purpose action `foreachRDD`. "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "99696609577F49DB809AF94C319CB449"
    },
    "cell_type" : "code",
    "source" : [ "val stableSparkSession = sparkSession\n", "import stableSparkSession.implicits._\n", "import org.apache.spark.sql.SaveMode.Append\n", "schemaStream.foreachRDD{rdd => \n", "                        val sensorDF = rdd.toDF()\n", "                        val sensorWithInfo = sensorDF.join(broadcast(sensorRef), \"sensorId\")\n", "                        val denormalizedSensorData =\n", "                            sensorWithInfo.withColumn(\"dnvalue\", $\"value\"*($\"maxRange\"-$\"minRange\")+$\"minRange\")\n", "                        val sensorRecords = denormalizedSensorData.drop(\"value\", \"maxRange\", \"minRange\")\n", "                        sensorRecords.write.format(\"parquet\").mode(Append).save(s\"$workDir/$targetFile\")\n", "                       }" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F366201F2275412F818532AB671A55BC"
    },
    "cell_type" : "code",
    "source" : [ "streamingContext.start()" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B6F0075E9BB04467858CABAA000489EF"
    },
    "cell_type" : "code",
    "source" : [ "// Be careful not to stop the context if you want the streaming process to continue\n", "// uncomment the statement below and execute this cell to stop the streaming process\n", "// streamingContext.stop(false)" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "5BF4B4ECDC794A769ED429A2D35B8A38"
    },
    "cell_type" : "markdown",
    "source" : "#Inspect the result\nWe can use the current Spark Session concurrently with the running Spark Streaming job in order to inspect the resulting data.\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "87973510A2E544B88D0825533CB24BC5"
    },
    "cell_type" : "code",
    "source" : [ "val enrichedRecords = sparkSession.read.parquet(s\"$workDir/$targetFile\")\n", "enrichedRecords" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "03C77BDE93904F3A8BDD12B66B427E5A"
    },
    "cell_type" : "code",
    "source" : [ "enrichedRecords.count" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2DF364CB97B8419095105116FC4D1897"
    },
    "cell_type" : "code",
    "source" : [ "enrichedRecords.count" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "EE4C39F5EBCF44BB81021B4B50015C68"
    },
    "cell_type" : "code",
    "source" : [ "" ],
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}