{
  "metadata" : {
    "id" : "a6321d91-1183-4799-8d64-3b9deecfb255",
    "name" : "streaming-data-to-parquet",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "40AAFF23B9654D2A9B2E0C59515BAF8A"
    },
    "cell_type" : "markdown",
    "source" : "# Writing Streaming Data to Parquet"
  }, {
    "metadata" : {
      "id" : "79CD566650664F04A8515F55A809A44D"
    },
    "cell_type" : "markdown",
    "source" : "## Our Streaming dataset will consist of sensor information, containing the sensorId, a timestamp, and a value.\nFor the sake of simplicity in this self-contained example, we are going to generate a randomized dataset, using an scenario that simulates a real IoT use case.\nThe timestamp will be the time of execution and each record will be formatted as a string coming from \"the field\" of comma separated values.\n\nWe also add a bit of real-world chaos to the data: Due to weather conditions, some sensors publish corrupt data. "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "6074E008266C4FDF92DFB936D47A7CBE"
    },
    "cell_type" : "code",
    "source" : "val sensorCount = 100000\nval workDir = \"/tmp/learningsparkstreaming/\"",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "9CA4FD2CB34F47AF8097D5E2A2BF97DA"
    },
    "cell_type" : "code",
    "source" : "import scala.util.Random\n\nval sensorId: () => Int = () =>  Random.nextInt(sensorCount) // 100K sensors in our system\nval data: () => Double = () => Random.nextDouble\nval timestamp: () => Long = () => System.currentTimeMillis\nval recordFunction: () => String = { () => \n                                      if (Random.nextDouble < 0.9) {\n                                        Seq(sensorId().toString, timestamp(), data()).mkString(\",\")\n                                      } else {\n                                        \"!!~corrupt~^&##$\"                                   \n                                      }\n                                   }",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "5806B402E3314F15A0B21F46BC97FF4A"
    },
    "cell_type" : "markdown",
    "source" : "### We use a particular trick that requires a moment of attention\nInstead of creating an RDD of text records, we create an RDD of record-generating functions. \nThen, each time the RDD is evaluated, the record function will generate a new random record. \nThis way we can simulate a realistic load of data that delivers a different set on each batch."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "A9AADE76D9DA471B9708B80B3E765890"
    },
    "cell_type" : "code",
    "source" : "val sensorDataGenerator = sparkContext.parallelize(1 to 100).map(_ => recordFunction)\nval sensorData = sensorDataGenerator.map(recordFun => recordFun())",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "9954BB2935D8408C9FC5F563BB108504"
    },
    "cell_type" : "markdown",
    "source" : "## Lets sample some data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1859881927-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "0D10D927B8F74EB88ECD6DB3A60A6064"
    },
    "cell_type" : "code",
    "source" : "sensorData.take(5)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "BFF9E80BAB5C4D468154BCD6FB5C6501"
    },
    "cell_type" : "markdown",
    "source" : "## We create our Streaming Context"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "83B2B58C552240BBA6FF518B1AD274EB"
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.streaming.StreamingContext\nimport org.apache.spark.streaming.Seconds\n\nval streamingContext = new StreamingContext(sparkContext, Seconds(2))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "4E6DBF82BB6F4B6584233CD460F67263"
    },
    "cell_type" : "markdown",
    "source" : "## Our stream source will be a ConstantInputDStream fed by the record-generating RDD.\nBy combining a constant input DStream with the record generating RDD, we create a changing stream of data to process in our example.\n(This method makes the example self-contained. It removes the need of an external stream generating process)\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "DF03F66BDDE0447B8202D39F2C0202E2"
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.streaming.dstream.ConstantInputDStream\nval rawDStream  = new ConstantInputDStream(streamingContext, sensorData)\n",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "CCCB597031E7451FB59D18BA85C0E4A4"
    },
    "cell_type" : "markdown",
    "source" : "# Providing Schema information for our streaming data\nNow that we have a DStream of fresh data processed in a 2-second interval, we can start focusing on the gist of this example.\nFirst, we want to define and apply a schema to the data we are receiving.\nIn Scala, we can define a schema with a `case class`"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "E7A917C393654969812E6E38223BBA52"
    },
    "cell_type" : "code",
    "source" : "case class SensorData(sensorId: Int, timestamp: Long, value: Double)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "9AD1ACAD450E44DA8C046EB48CD4EE5A"
    },
    "cell_type" : "markdown",
    "source" : "Now we apply that schema to the dstream, using the `flatMap` function.\n\nWe use `flatMap` instead of a `map` because there might be cases when the incoming data is incomplete or corrupted.\nIf we would use `map`, we would have to provide a resulting value for each transformed record. \nThat is something we cannot do for invalid records.\nWith `flatMap` in combination with `Option`, we can represent valid records as `Some(recordValue)` and invalid records as `None`.\nBy the virtue of `flatMap` the internal `Option` container gets flattend and our resulting stream will only contain valid `recordValue`s.\n\nDuring the parsing of the comma separated records, we not only protect ourselves against missing fields, but also parse the numeric values to their expected types. The surrounding `Try` captures any `NumberFormatException` that might arise from invalid records."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "5285C2BBC1854F059AB8E1D0244AE1C7"
    },
    "cell_type" : "code",
    "source" : "import scala.util.Try\nval schemaStream = rawDStream.flatMap{record => \n                                  val fields = record.split(\",\")\n                                  if (fields.size == 3) {\n                                    Try (SensorData(fields(0).toInt, fields(1).toLong, fields(2).toDouble)).toOption\n                                  } else { None }\n                                 }",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "7A7C144384904E96BE66A649BD193C15"
    },
    "cell_type" : "markdown",
    "source" : "# Saving DataFrames\nWith the schema stream in place, we can proceed to transform the underlying RDDs in DataFrames.\nWe do this in the context of the general-purpose action `foreachRDD`. \nIt's impossible to use transformation at this point because a `DStream[DataFrame]` is undefined.\nThis also means that any further operations we would like to apply to the DataFrame (or DataSet) needs to be contained in the scope of the \n`foreachRDD` closure."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "99696609577F49DB809AF94C319CB449"
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.sql.SaveMode.Append\nschemaStream.foreachRDD{rdd => \n                        val df = rdd.toDF()\n                        df.write.format(\"parquet\").mode(Append).save(s\"$workDir/iotstream.parquet\")\n                       }",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "F366201F2275412F818532AB671A55BC"
    },
    "cell_type" : "code",
    "source" : "streamingContext.start()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "B6F0075E9BB04467858CABAA000489EF"
    },
    "cell_type" : "code",
    "source" : "streamingContext.stop(false)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "87973510A2E544B88D0825533CB24BC5"
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}
